<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR39LSGTSL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PR39LSGTSL');
  </script>
  
  <meta charset="utf-8">
  <meta name="description"
        content="ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots">
  <meta name="keywords" content="LearningfromDemonstration, Manipulation, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item">
        <span class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"> <path fill="#808080" d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"/></svg>      
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/AdrianPrados/Robotic-Rearrangement-of-Everyday-Objects" target="_blank">
              Everyday Objects Rearrangement in a Human-Like Manner via Robotic Imagination and Learning from Demonstration
            </a>
            <a class="navbar-item" href="https://github.com/AdrianPrados/Learning-and-generalization-of-task-parameterized-skills-through-few-human-demonstrations" target="_blank">
              Learning and generalization of task-parameterized skills through few human demonstrations
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/LogoJornada.png" alt="JornadasAutomatica2025" style="width: 160px;;height:auto;">
          <h1 class="title is-1 publication-title">ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots</h1>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a href="https://github.com/AdrianPrados" target="_blank"> Adrian Prados,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/gonecho" target="_blank"> Gonzalo Espinoza,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/albmendez" target="_blank"> Alberto Mendez,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/almorav" target="_blank"> Alicia Mora,</a>
            </span>

            <span class="author-block">
              <a href="https://scholar.google.es/citations?user=zVLybhsAAAAJ&hl=es" target="_blank"> Ramon Barber</a>
            </span>

            <!--span class="author-block">
              <a href="" target="_blank"> Author 6</a>
            </span-->
          </div>

          <div class="is-size-5 publication-authors">
            <a class="author-block" href="https://mobile-robots-group-uc3m.github.io/MobileRobotsDocumentation/" target="_blank">Mobile Robots Group, RoboticsLab, University Carlos III</a>
          </div>

          <!--div class="is-size-5 publication-authors">
            <a class="author-block" href="" target="_blank">Laboratory 2</a>
          </div-->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!--<span class="link-block">
                <a href="https://arxiv.org/abs/2403.12533" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->

              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Mobile-Robots-Group-UC3M/AdamSim"
                   class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero body">
  <div class="container is-max-desktop">
    <img src="./static/images/ModulosADAM.png">
    <h2 class="subtitle has-text-centered">
      General integration of the modules developed for ADAMSim.
    </h2>
  </div>
</section>


<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper introduces ADAMSim, a PyBullet-based simulation environment tailored for Ambidextrous Domestic Autonomous Manipulator (ADAM), 
            developed to support research in navigation, manipulation, and learning for domestic robotics. The simulator accurately replicates the 
            structure and behavior of the physical robot, enabling robust sim-to-real and real-to-sim algorithm transfer. ADAMSim follows a modular design, 
            including navigation, arm and hand kinematics, perception, and ROS communication. This architecture allows synchronized operation between the 
            real robot and its digital twin. Several example applications were developed, ranging from vision and grasping tasks to navigation and teleoperation, 
            including experiments running both simulated and real robots simultaneously. Its open-source and flexible design makes ADAMSim a powerful tool for 
            safe and reproducible algorithm development and experimentation in household robotics. The platform is also intended to support future research in 
            indoor mapping, advanced manipulation learning, and educational projects, serving as a test bed.
          </p>
        </div>
      </div>
    </div>
    <div class="column">
      <img src="./static/images/RealSim.png">
      <p class="subtitle has-text-centered">
        (a) Learning a manipulation task from demonstrations using a real-sim-real pipeline, 
        (b) Simultaneous navigation with ADAM real and simulated environments
      </p>
    </div>
  </div>
  </div>
</section>



<!-- Abstract. -->
<!--section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robots can learn skills through a variety of methods, such as Learning from Demonstration (LfD) or Reinforcement Learning (RL). 
            These methods typically learn some generalization capabilities, but cannot adapt quickly to large changes in the environment. 
            We propose a novel adaptation method that combines LfD techniques with visual feedback to accurately adjust task execution. 
            This method relies on learning a velocity map representation of the task from demonstrations, which encodes the general form of the skill. 
            Then, this encoding can be adapted to learn a reproduction quickly using information detected visually from the environment. 
            We perform our adaptation method in several real-world environments using a dual-arm robot platform.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-narrow">
            <img src="./static/images/IntroPaper.png" width="20%">
            <p class="subtitle has-text-centered">
              Our bimanual robot performing a task-oriented learning process to rearrange the table. 
              The algorithm detects objects using the camera, calculates the optimal grasping points, 
              and learns both the position and orientation of the entire task using the VASO algorithm, 
              which learns the velocity space through demonstrations and adjusts the solution using the waypoints generalizing 
              the learned model.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  <<div class="container is-max-desktop">
    <div class="hero body">
      <video id="teaser" autoplay="autoplay" controls autoplay muted loop playsinline height="100%">
        <source src="./static/videos/IROS24-website.mp4" type="video/mp4">
      </video>
      <p class="subtitle has-text-centered">
        ADRI AQUI METE EL VIDEO
      </p>
    </div>
  </div>>
</section-->



<!-- ADAMSim Modules -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">ADAMSim Modules</h2>
        <div class="content has-text-justified">
          <p>
            <strong>ADAMSim</strong> is structured in a modular fashion, allowing flexibility, scalability, and ease of integration with both simulated and real robotic systems. The core modules are:
          </p>
        </div>
      </div>
    </div>

    <!-- Navigation Module -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            <strong>Navigation Module:</strong> Enables control of the mobile base, supporting both manual teleoperation and autonomous waypoint navigation. It includes continuous movement controllers, real-time odometry tracking, and customizable speed settings. Obstacle insertion and detection is also supported, allowing experiments with dynamic path planning.
          </p>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="hero body">
        <img src="./static/images/NavigationImg.jpg" alt="Navigation Module">
        <p class="subtitle has-text-centered">
          (a) ADAM’s motion is defined by the linear velocity <em>v</em>, angular velocity <em>&omega;</em>, 
          and wheel velocities <em>v<sub>r</sub></em> and <em>v<sub>l</sub></em>. 
          <em>&theta;</em> determines the robot’s orientation.<br>
          (b) Example of use of the Lidar information for navigation. Red color represents obstacles 
          (purple and yellow boxes) in the ray casting.
        </p>
      </div>
    </div>

    <!-- Arm and Hand Kinematics Module -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            <strong>Arm and Hand Kinematics Module:</strong> Provides full kinematic control of both arms and grippers. Users can command the robot using joint angles or Cartesian goals, with inverse kinematics and trajectory generation included. It supports both high-level (goal reaching) and low-level joint manipulation.
          </p>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="hero body">
        <img src="./static/images/example_armposes.png" alt="Arm Kinematics Module">
        <p class="subtitle has-text-centered">
          Sequence of movements of the right arm passing through different waypoints with specific position and orientation.
        </p>
      </div>
    </div>

    <!-- Perception Module -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            <strong>Perception Module:</strong> Simulates onboard RGB-D sensors and provides synthetic visual data. This is ideal for training vision models and performing object detection, segmentation, and grasp planning using realistic 3D input.
          </p>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="hero body">
        <img src="./static/images/VisionExample.png" alt="Perception Module">
        <p class="subtitle has-text-centered">
          Example of vision with the RGB-D sensor. ADAMSim provides RGB information, depth, and segmented masks in a synthetic manner.
        </p>
      </div>
    </div>

    <!-- ROS Communication Module -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            <strong>ROS Communication Module:</strong> Creates a bridge between the simulated and real robots via ROS. This module allows for synchronized execution and bidirectional data transfer between ADAMSim and the physical ADAM robot, supporting both sim-to-real and real-to-sim experiments.
          </p>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="hero body">
        <img src="./static/images/ROS Scheme.pdf" alt="ROS Communication Module">
        <p class="subtitle has-text-centered">
          Diagram of the connections between the real model and the simulator through the created ROS bridge.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- Examples of the method. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Comparison of VASO against other methods</h2>
        <div class="content has-text-justified">
          <p>
            To evaluate the performance of the VASO algorithm, a series of comparative experiments were conducted against state-of-the-art methods, 
            including NDF, R-NDF, NIFT, and MIMO. While the former three relied on manually annotated datasets for training, VASO and MIMO 
            did not require such annotations. All methods were trained and executed under the same hardware conditions, using an MSI Katana GF66 
            with an Intel i7 CPU and RTX 3070 GPU, and were evaluated using a single fixed camera viewpoint to match real-world conditions. 
            Two demonstration settings were tested: one-shot learning with a single demonstration (D1) and learning from multiple demonstrations (D10). 
            VASO utilized kinesthetic demonstrations, while others relied on visual data. The experiments covered six pick-and-place tasks involving 
            novel objects in varying positions and orientations. Performance was assessed through grasp success (G<sub>s</sub>), placement success (P<sub>s</sub>), 
            and overall task success (O<sub>s</sub>), along with angular error between the predicted and ideal poses. Across all tasks, VASO consistently 
            outperformed the other methods in both D1 and D10 settings, showing higher success rates and lower angular error. Notably, it achieved robust 
            performance regardless of the number of demonstrations, highlighting its reliability for tasks requiring both positional and orientational 
            precision.
          </p>
        </div>
      </div>
    </div>
    <div class="column">
      <img src="./static/images/Tabla.png">
      <p class="subtitle has-text-centered">
        Success rates among the compared methods for different manipulation tasks with unseen objects. It can be seen how our method VASO generate better results 
        than the other methods, even with only one demonstration. The method MIMO is the only one that can be compared with VASO, but it needs 10 demonstrations to
        achieve similar results.
      </p>
    </div>
    <!-- Video Section -->
  <!-- <div class="container">
    <div class="columns is-multiline is-centered">
      
      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp1IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 1: Opposite movements without obstacles.</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp2IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 2: Opposite movements with 1 obstacles.</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp3IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 3: Opposite movements with 5 obstacles (narrow passage).</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp4IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 4: Obstacle in final point.</p>
      </div> -->

    </div>
  </div>
  </div>
</section>


<!-- Examples with ADAM. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Real world examples using VASO</h2>
        <div class="content has-text-justified">
          <p>
            To assess the capabilities of the VASO algorithm, a series of real-world experiments were carried out using a mobile dual-arm robot 
            equipped with two UR3 arms, a Realsense D435i RGB-D camera, and two Inspire Robots RH56DFX robotic hands. The experiments involved 
            tasks related to object reorganization and liquid serving, classified by complexity (single-arm or dual-arm), length (short or long), 
            and type (e.g., placing, stacking, serving). Seven tasks (additionally for the ones presented in the comparisons with other methods) 
            were performed, including object placement on shelves, liquid pouring with 
            and without obstacles, stacking, and simultaneous object handling. While the final positions for object placement and serving were 
            predefined, VASO autonomously determined grasping points, motion sequences, and task order based on object detection, showcasing its 
            ability to plan and execute complex, adaptive dual-arm manipulation tasks.The robot successfully adapted to the environment in real 
            time, generating efficient grasping poses for both arms. The algorithm demonstrated its effectiveness in learning the task through 
            kinesthetic demonstrations and adapting to the environment using visual input. The results highlight VASO's potential for real-world 
            applications in robotic manipulation tasks.

          </p>
        </div>
      </div>
    </div>
    <!-- Video Section -->
  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 1 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/TareaCortaObjetosinBox.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object placement task using one arm into a box.</p>
      </div>

      <!-- Video 2 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/TareaBrazoIzq.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object placement task using one arm over a shelf.</p>
      </div>

    </div>
  </div>

  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 3 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/StackObjetcs.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object stacking task.</p>
      </div>

      <!-- Video 4 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/ServirAgua.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Water serving task in different scenarios using both arms.</p>
      </div>

    </div>
  </div>

  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 5 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/DosbrazosCajas.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object picking task placing them into a box using both arms.</p>
      </div>

      <!-- Video 6 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/DosBrazosoverBox.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object picking task placing them over a shelf using both arms.</p>
      </div>

    </div>
  </div>
  </div>

  

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template.
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
<script>hljs.highlightAll();</script>

<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PR39LSGTSL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PR39LSGTSL');
  </script>
  
  <meta charset="utf-8">
  <meta name="description"
        content="ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots">
  <meta name="keywords" content="LearningfromDemonstration, Manipulation, Robot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/styles/default.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.3.1/highlight.min.js"></script>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item">
        <span class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"> <path fill="#808080" d="M320 0c17.7 0 32 14.3 32 32V96H472c39.8 0 72 32.2 72 72V440c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72H288V32c0-17.7 14.3-32 32-32zM208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H208zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H304zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16H400zM264 256a40 40 0 1 0 -80 0 40 40 0 1 0 80 0zm152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80zM48 224H64V416H48c-26.5 0-48-21.5-48-48V272c0-26.5 21.5-48 48-48zm544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48H576V224h16z"/></svg>      
        </span>
        </a>
  
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="" target="_blank">
              Not available now
            </a>
            <a class="navbar-item" href="" target="_blank">
              Not available now
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/Humanoids-2025.png" alt="iros-25" style="width:160px;height:auto;">
          <h1 class="title is-1 publication-title">ADAMSim: PyBullet-Based Simulation Environment for Research on Domestic Mobile Manipulator Robots</h1>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a href="https://github.com/AdrianPrados" target="_blank"> Adrian Prados,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/gonecho" target="_blank"> Gonzalo Espinoza,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/albmendez" target="_blank"> Alberto Mendez,</a>
            </span>

            <span class="author-block">
              <a href="https://github.com/almorav" target="_blank"> Alicia Mora</a>
            </span>

            <span class="author-block">
              <a href="https://scholar.google.es/citations?user=zVLybhsAAAAJ&hl=es" target="_blank"> Ramon Barber</a>
            </span>

            <!--span class="author-block">
              <a href="" target="_blank"> Author 6</a>
            </span-->
          </div>

          <div class="is-size-5 publication-authors">
            <a class="author-block" href="https://mobile-robots-group-uc3m.github.io/MobileRobotsDocumentation/" target="_blank">Mobile Robots Group, RoboticsLab, University Carlos III</a>
          </div>

          <!--div class="is-size-5 publication-authors">
            <a class="author-block" href="" target="_blank">Laboratory 2</a>
          </div-->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!--<span class="link-block">
                <a href="https://arxiv.org/abs/2403.12533" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->

              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Mobile-Robots-Group-UC3M/AdamSim"
                   class="external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero body">
  <div class="container is-max-desktop">
    <img src="./static/images/GeneralScheme.png">
    <h2 class="subtitle has-text-centered">
      Our algorithm takes as input the vision data (a), which is decomposed into 2D information‚Äîused to segment 
      the relevant object classes‚Äîand 3D information for depth estimation. Both are then fused and encoded as BPS (b) for each detected object 
      in the environment ùë•<sub>b</sub>. This encoding is used to estimate potential grasps via a modified version of FFHNet (c), 
      where pretrained weights are leveraged to estimate the latent space ùëß used to generate grasp poses for the robotic hand. 
      These grasps are subsequently optimized based on the reachability constraints of both the hand and the robotic arm. The full 3D scene ùí™ 
      is used to learn the traversable regions (d), along with the user's demonstration. The generated waypoints ùêñ are used to adapt the learned models ùêÖ‚Çì 
      and ùêÖ<sub>ùúÉ</sub>, producing a motion solution that passes through all the waypoints (e), while adapting their learned positions and orientations.

    </h2>
  </div>
</section>


<!-- Abstract. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper introduces VASO (Visual Adaptation of Skills in One-Shot), a novel framework that enables robots to adapt
            learned manipulation skills to new environments using a single demonstration. VASO leverages visual keypoint-based
            representations and integrates them with a trajectory modeling approach inspired by the Fast Marching Square
            method to generate safe, obstacle-aware paths. It incorporates object detection from visual data and 3D point
            clouds to identify key manipulation points, applying a Variational Autoencoder to determine optimal grasp
            positions and orientations. The learned skill is then adapted through an Elastic Map-inspired deformation
            technique, enabling task generalization with respect to new via-points or constraints. Unlike existing visual imitation
            learning methods, VASO achieves robust one-shot generalization without iterative re-planning, and is
            validated in both simulated and real-world scenarios, demonstrating effectiveness in dynamic and cluttered environments. 
          </p>
        </div>
      </div>
    </div>
    <div class="column">
      <img src="./static/images/IntroPaper.png">
      <p class="subtitle has-text-centered">
        Our bimanual robot performing a task-oriented learning process to rearrange the table. 
        The algorithm detects objects using the camera, calculates the optimal grasping points, 
        and learns both the position and orientation of the entire task using the VASO algorithm, 
        which learns the velocity space through demonstrations and adjusts the solution using the waypoints generalizing 
        the learned model.
      </p>
    </div>
  </div>
  </div>
</section>



<!-- Abstract. -->
<!--section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robots can learn skills through a variety of methods, such as Learning from Demonstration (LfD) or Reinforcement Learning (RL). 
            These methods typically learn some generalization capabilities, but cannot adapt quickly to large changes in the environment. 
            We propose a novel adaptation method that combines LfD techniques with visual feedback to accurately adjust task execution. 
            This method relies on learning a velocity map representation of the task from demonstrations, which encodes the general form of the skill. 
            Then, this encoding can be adapted to learn a reproduction quickly using information detected visually from the environment. 
            We perform our adaptation method in several real-world environments using a dual-arm robot platform.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-narrow">
            <img src="./static/images/IntroPaper.png" width="20%">
            <p class="subtitle has-text-centered">
              Our bimanual robot performing a task-oriented learning process to rearrange the table. 
              The algorithm detects objects using the camera, calculates the optimal grasping points, 
              and learns both the position and orientation of the entire task using the VASO algorithm, 
              which learns the velocity space through demonstrations and adjusts the solution using the waypoints generalizing 
              the learned model.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  <<div class="container is-max-desktop">
    <div class="hero body">
      <video id="teaser" autoplay="autoplay" controls autoplay muted loop playsinline height="100%">
        <source src="./static/videos/IROS24-website.mp4" type="video/mp4">
      </video>
      <p class="subtitle has-text-centered">
        ADRI AQUI METE EL VIDEO
      </p>
    </div>
  </div>>
</section-->



<!-- System. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Method Description</h2>
        <div class="content has-text-justified">
          <p>
            We propose a novel method, known as <em>Visual Adaptation of Skills in One-Shot</em> (VASO), 
            which applies the ideas of visual keypoint-based representations in a one-shot manner. 
            Our method takes one or more demonstrations and encodes them into a skill model. 
            This skill model is inspired by the Fast Marching Square method, which models the demonstrations as light paths through the workspace. 
            This model encodes "safe" paths of the skill through the workspace, avoiding any obstacles. 
            Once the skill model is obtained, the algorithm detects objects of interest using a detection system based on the overlap of the object 
            masks onto their 3D point cloud. This allows us to detect the keypoints of obstacles or objects of interest within the workspace, 
            enabling the application of an algorithm based on Variational Autoencoders (VAE) that estimates both the optimal position and orientation 
            for grasping the object, based on its reachability. A path through the workspace is generated which avoids any obstacles. 
            Then, this path is modified to meet any necessary via-points (which may be position and/or orientation via-points) using a method 
            inspired by elastic maps. This modified path can be used as a successful reproduction of the task through the workspace, 
            without the need for iteration. Notably, this method can handle one or more demonstrations, can adapt to any number of novel initial, 
            final, via-point, or obstacle constraints, and generate a successful reproduction efficiently. We evaluate our approach in several 
            real-world environments using a bimanual manipulator robot.
          </p>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
    <div class="hero body">
      <video id="teaser" autoplay="autoplay" controls autoplay muted loop playsinline height="100%">
        <source src="./static/videos/VideoLargoHumanoids.mp4" type="video/mp4">
      </video>
      <p class="subtitle has-text-centered">
        Example video showing the VASO algorithm and its functionality for shelf placement tasks and object rearrangement in a box.
        The algorithm is capable of learning the task through kinesthetic demonstrations and adapting to the environment in real time using the visual input
        and generating efficient grasping poses for both of the arms. 
      </p>
    </div>
  </div>
    <div class="content has-text-justified">
      <!--p>
        Explain the method a little bit more :3 
      </p-->
    </div>
  </div>
</section>

<!-- Examples of the method. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Comparison of VASO against other methods</h2>
        <div class="content has-text-justified">
          <p>
            To evaluate the performance of the VASO algorithm, a series of comparative experiments were conducted against state-of-the-art methods, 
            including NDF, R-NDF, NIFT, and MIMO. While the former three relied on manually annotated datasets for training, VASO and MIMO 
            did not require such annotations. All methods were trained and executed under the same hardware conditions, using an MSI Katana GF66 
            with an Intel i7 CPU and RTX 3070 GPU, and were evaluated using a single fixed camera viewpoint to match real-world conditions. 
            Two demonstration settings were tested: one-shot learning with a single demonstration (D1) and learning from multiple demonstrations (D10). 
            VASO utilized kinesthetic demonstrations, while others relied on visual data. The experiments covered six pick-and-place tasks involving 
            novel objects in varying positions and orientations. Performance was assessed through grasp success (G<sub>s</sub>), placement success (P<sub>s</sub>), 
            and overall task success (O<sub>s</sub>), along with angular error between the predicted and ideal poses. Across all tasks, VASO consistently 
            outperformed the other methods in both D1 and D10 settings, showing higher success rates and lower angular error. Notably, it achieved robust 
            performance regardless of the number of demonstrations, highlighting its reliability for tasks requiring both positional and orientational 
            precision.
          </p>
        </div>
      </div>
    </div>
    <div class="column">
      <img src="./static/images/Tabla.png">
      <p class="subtitle has-text-centered">
        Success rates among the compared methods for different manipulation tasks with unseen objects. It can be seen how our method VASO generate better results 
        than the other methods, even with only one demonstration. The method MIMO is the only one that can be compared with VASO, but it needs 10 demonstrations to
        achieve similar results.
      </p>
    </div>
    <!-- Video Section -->
  <!-- <div class="container">
    <div class="columns is-multiline is-centered">
      
      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp1IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 1: Opposite movements without obstacles.</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp2IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 2: Opposite movements with 1 obstacles.</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp3IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 3: Opposite movements with 5 obstacles (narrow passage).</p>
      </div>

      
      <div class="column is-one-quarter">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/Exp4IIWA.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Example 4: Obstacle in final point.</p>
      </div> -->

    </div>
  </div>
  </div>
</section>


<!-- Examples with ADAM. -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Real world examples using VASO</h2>
        <div class="content has-text-justified">
          <p>
            To assess the capabilities of the VASO algorithm, a series of real-world experiments were carried out using a mobile dual-arm robot 
            equipped with two UR3 arms, a Realsense D435i RGB-D camera, and two Inspire Robots RH56DFX robotic hands. The experiments involved 
            tasks related to object reorganization and liquid serving, classified by complexity (single-arm or dual-arm), length (short or long), 
            and type (e.g., placing, stacking, serving). Seven tasks (additionally for the ones presented in the comparisons with other methods) 
            were performed, including object placement on shelves, liquid pouring with 
            and without obstacles, stacking, and simultaneous object handling. While the final positions for object placement and serving were 
            predefined, VASO autonomously determined grasping points, motion sequences, and task order based on object detection, showcasing its 
            ability to plan and execute complex, adaptive dual-arm manipulation tasks.The robot successfully adapted to the environment in real 
            time, generating efficient grasping poses for both arms. The algorithm demonstrated its effectiveness in learning the task through 
            kinesthetic demonstrations and adapting to the environment using visual input. The results highlight VASO's potential for real-world 
            applications in robotic manipulation tasks.

          </p>
        </div>
      </div>
    </div>
    <!-- Video Section -->
  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 1 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/TareaCortaObjetosinBox.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object placement task using one arm into a box.</p>
      </div>

      <!-- Video 2 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/TareaBrazoIzq.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object placement task using one arm over a shelf.</p>
      </div>

    </div>
  </div>

  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 3 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/StackObjetcs.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object stacking task.</p>
      </div>

      <!-- Video 4 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/ServirAgua.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Water serving task in different scenarios using both arms.</p>
      </div>

    </div>
  </div>

  <div class="container">
    <div class="columns is-centered">
      
      <!-- Video 5 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/DosbrazosCajas.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object picking task placing them into a box using both arms.</p>
      </div>

      <!-- Video 6 -->
      <div class="column is-half">
        <figure class="image is-16by9">
          <video controls>
            <source src="./static/videos/DosBrazosoverBox.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </figure>
        <p class="has-text-centered">Object picking task placing them over a shelf using both arms.</p>
      </div>

    </div>
  </div>
  </div>

  

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template.
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
<script>hljs.highlightAll();</script>
